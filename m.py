# -*- coding: utf-8 -*-
"""v2-ML Model-bangalore_house_price_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XWPm8WX4DN7ig9rUPXwC1lV2azVn44O2

# Bangalore House Price Prediction - Outlier Detection

This notebook only train ML model on different ml algorithms
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", None)

"""from google.colab import files
files=files.upload()
df = pd.read_csv('oh_encoded_data.csv')"""

# Get clean data
path = r"https://drive.google.com/uc?export=download&id=1P49POlAk27uRzWKXoR2WaEfb1lyyfiRJ"  # oh_encoded_data.csv from drive

# This file contain [area_type  availability    location    bath    balcony price   total_sqft_int  bhk price_per_sqft]
# and ['area_type','availability','location'] this are cat var
# We encoded few classes from above car var in OHE

df = pd.read_csv(path)
df.shape

df.shape

df.head()

df = df.drop(['Unnamed: 0'], axis=1)
df.head()

df.shape

"""## Split Dataset in train and test"""

X = df.drop("price", axis=1)
y = df['price']
print('Shape of X = ', X.shape)
print('Shape of y = ', y.shape)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=51)
print('Shape of X_train = ', X_train.shape)
print('Shape of y_train = ', y_train.shape)
print('Shape of X_test = ', X_test.shape)
print('Shape of y_test = ', y_test.shape)

"""## Feature Scaling"""

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
sc.fit(X_train)
X_train = sc.transform(X_train)
X_test = sc.transform(X_test)

"""## Machine Learning Model Training

## Linear Regression
"""

from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error

lr = LinearRegression()
lr_lasso = Lasso()
lr_ridge = Ridge()


def rmse(y_test, y_pred):
    return np.sqrt(mean_squared_error(y_test, y_pred))


lr.fit(X_train, y_train)
lr_score = lr.score(X_test, y_test)  # with all num var 0.7842744111909903
lr_rmse = rmse(y_test, lr.predict(X_test))
lr_score, lr_rmse

# Lasso
lr_lasso.fit(X_train, y_train)
lr_lasso_score = lr_lasso.score(X_test, y_test)  # with balcony 0.5162364637824872
lr_lasso_rmse = rmse(y_test, lr_lasso.predict(X_test))
lr_lasso_score, lr_lasso_rmse

"""## Support Vector Machine"""

from sklearn.svm import SVR

svr = SVR()
svr.fit(X_train, y_train)
svr_score = svr.score(X_test, y_test)  # with 0.2630802200711362
svr_rmse = rmse(y_test, svr.predict(X_test))
svr_score, svr_rmse

"""## Random Forest Regressor"""

from sklearn.ensemble import RandomForestRegressor

rfr = RandomForestRegressor()
rfr.fit(X_train, y_train)
rfr_score = rfr.score(X_test, y_test)  # with 0.8863376025408044
rfr_rmse = rmse(y_test, rfr.predict(X_test))
rfr_score, rfr_rmse

"""## XGBoost"""

import xgboost

xgb_reg = xgboost.XGBRegressor()
xgb_reg.fit(X_train, y_train)
xgb_reg_score = xgb_reg.score(X_test, y_test)  # with 0.8838865742273464
xgb_reg_rmse = rmse(y_test, xgb_reg.predict(X_test))
xgb_reg_score, xgb_reg_rmse

print(pd.DataFrame([{'Model': 'Linear Regression', 'Score': lr_score, "RMSE": lr_rmse},
                    {'Model': 'Lasso', 'Score': lr_lasso_score, "RMSE": lr_lasso_rmse},
                    {'Model': 'Support Vector Machine', 'Score': svr_score, "RMSE": svr_rmse},
                    {'Model': 'Random Forest', 'Score': rfr_score, "RMSE": rfr_rmse},
                    {'Model': 'XGBoost', 'Score': xgb_reg_score, "RMSE": xgb_reg_rmse}],
                   columns=['Model', 'Score', 'RMSE']))

"""## Cross Validation"""

'''from sklearn.model_selection import KFold,cross_val_score
cvs = cross_val_score(xgb_reg, X_train,y_train, cv = 10)
cvs, cvs.mean() # 0.9845963377450353)'''

'''cvs_rfr = cross_val_score(rfr, X_train,y_train, cv = 10)
cvs_rfr, cvs_rfr.mean() # 0.9652425691235843)'''

from sklearn.model_selection import cross_val_score

cvs_rfr2 = cross_val_score(RandomForestRegressor(), X_train, y_train, cv=10)
cvs_rfr2, cvs_rfr2.mean()  # 0.9652425691235843)'''

"""# Hyper Parmeter Tuning"""

from sklearn.model_selection import GridSearchCV
from xgboost.sklearn import XGBRegressor

'''
# Various hyper-parameters to tune
xgb1 = XGBRegressor()
parameters = {'learning_rate': [0.1,0.03, 0.05, 0.07], #so called `eta` value, # [default=0.3] Analogous to learning rate in GBM
              'min_child_weight': [1,3,5], #[default=1] Defines the minimum sum of weights of all observations required in a child.
              'max_depth': [4, 6, 8], #[default=6] The maximum depth of a tree,
              'gamma':[0,0.1,0.001,0.2], #Gamma specifies the minimum loss reduction required to make a split.
              'subsample': [0.7,1,1.5], #Denotes the fraction of observations to be randomly samples for each tree.
              'colsample_bytree': [0.7,1,1.5], #Denotes the fraction of columns to be randomly samples for each tree.
              'objective':['reg:linear'], #This defines the loss function to be minimized.

              'n_estimators': [100,300,500]}

xgb_grid = GridSearchCV(xgb1,
                        parameters,
                        cv = 2,
                        n_jobs = -1,
                        verbose=True)

xgb_grid.fit(X_train, y_train)

print(xgb_grid.best_score_) # 0.9397345161940295
print(xgb_grid.best_params_)'''

'''xgb_tune = xgb_grid.estimator

xgb_tune.fit(X_train,y_train) # 0.9117591385438816
xgb_tune.score(X_test,y_test)'''

'''cvs = cross_val_score(xgb_tune, X_train,y_train, cv = 10)
cvs, cvs.mean() #  0.9645582338461773)'''

# [i/10.0 for i in range(1,6)]

# xgb_grid.estimator

xgb_tune2 = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
                         colsample_bynode=0.6, colsample_bytree=1, gamma=0,
                         importance_type='gain', learning_rate=0.25, max_delta_step=0,
                         max_depth=4, min_child_weight=1, missing=None, n_estimators=400,
                         n_jobs=1, nthread=None, objective='reg:linear', random_state=0,
                         reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
                         silent=None, subsample=1, verbosity=1)
xgb_tune2.fit(X_train, y_train)  # 0.9412851220926807
xgb_tune2.score(X_test, y_test)

'''parameters = {'learning_rate': [0.1,0.03, 0.05, 0.07], #so called `eta` value, # [default=0.3] Analogous to learning rate in GBM
              'min_child_weight': [1,3,5], #[default=1] Defines the minimum sum of weights of all observations required in a child.
              'max_depth': [4, 6, 8], #[default=6] The maximum depth of a tree,
              'gamma':[0,0.1,0.001,0.2], #Gamma specifies the minimum loss reduction required to make a split.
              'subsample': [0.7,1,1.5], #Denotes the fraction of observations to be randomly samples for each tree.
              'colsample_bytree': [0.7,1,1.5], #Denotes the fraction of columns to be randomly samples for each tree.
              'objective':['reg:linear'], #This defines the loss function to be minimized.
              'n_estimators': [100,300,500]}'''

xgb_tune2 = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
                         colsample_bynode=0.9, colsample_bytree=1, gamma=0,
                         importance_type='gain', learning_rate=0.05, max_delta_step=0,
                         max_depth=4, min_child_weight=5, missing=None, n_estimators=100,
                         n_jobs=1, nthread=None, objective='reg:linear', random_state=0,
                         reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
                         silent=None, subsample=1, verbosity=1)
xgb_tune2.fit(X_train, y_train)  # 0.9412851220926807
xgb_tune2.score(X_test, y_test)

cvs = cross_val_score(xgb_tune2, X_train, y_train, cv=5)
cvs, cvs.mean()  # 0.9706000326331659'''

np.sqrt(mean_squared_error(y_test, xgb_tune2.predict(X_test)))

"""## Test Model"""

list(X.columns)


# it help to get predicted value of hosue  by providing features value
def predict_house_price(model, bath, balcony, total_sqft_int, bhk, price_per_sqft, area_type, availability, location):
    x = np.zeros(len(X.columns))  # create zero numpy array, len = 107 as input value for model

    # adding feature's value accorind to their column index
    x[0] = bath
    x[1] = balcony
    x[2] = total_sqft_int
    x[3] = bhk
    x[4] = price_per_sqft

    if "availability" == "Ready To Move":
        x[8] = 1

    if 'area_type' + area_type in X.columns:
        area_type_index = np.where(X.columns == "area_type" + area_type)[0][0]
        x[area_type_index] = 1

        # print(area_type_index)

    if 'location_' + location in X.columns:
        loc_index = np.where(X.columns == "location_" + location)[0][0]
        x[loc_index] = 1

        # print(loc_index)

    # print(x)

    # feature scaling
    x = sc.transform([x])[0]  # give 2d np array for feature scaling and get 1d scaled np array
    # print(x)

    return model.predict([x])[0]  # return the predicted value by train XGBoost model


predict_house_price(model=xgb_tune2, bath=3, balcony=2, total_sqft_int=1672, bhk=3, price_per_sqft=8971.291866,
                    area_type="Plot  Area", availability="Ready To Move", location="Devarabeesana Halli")

##test sample
# area_type  availability    location    bath    balcony price   total_sqft_int  bhk price_per_sqft
# 2  Super built-up Area Ready To Move   Devarabeesana Halli 3.0 2.0 150.0   1750.0  3   8571.428571

predict_house_price(model=xgb_tune2, bath=3, balcony=2, total_sqft_int=1750, bhk=3, price_per_sqft=8571.428571,
                    area_type="Super built-up", availability="Ready To Move", location="Devarabeesana Halli")

##test sample
# area_type  availability    location    bath    balcony price   total_sqft_int  bhk price_per_sqft
# 1  Built-up Area   Ready To Move   Devarabeesana Halli 3.0 3.0 149.0   1750.0  3   8514.285714
predict_house_price(model=xgb_tune2, bath=3, balcony=3, total_sqft_int=1750, bhk=3, price_per_sqft=8514.285714,
                    area_type="Built-up Area", availability="Ready To Move", location="Devarabeesana Halli")

"""# Save model & load model"""

import joblib

# save model
joblib.dump(xgb_tune2, 'bangalore_house_price_prediction_model.pkl')
joblib.dump(rfr, 'bangalore_house_price_prediction_rfr_model.pkl')

# load model
bangalore_house_price_prediction_model = joblib.load("bangalore_house_price_prediction_model.pkl")

# predict house price
predict_house_price(bangalore_house_price_prediction_model, bath=3, balcony=3, total_sqft_int=150, bhk=3,
                    price_per_sqft=8514.285714, area_type="Built-up Area", availability="Ready To Move",
                    location="Devarabeesana Halli")